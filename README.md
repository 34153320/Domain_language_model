# Domain_language_model_I

Language models training on large scale dataset (for instance, BERT, XLnet, and GPT2) work for general purpose. However, in our daily application the language models are frequently used in specific domain. To avoid large scale retraining, and also transfer the pre-trained learned LM, here a strategy is proposed to obtain domain language model based on GPT2. 
